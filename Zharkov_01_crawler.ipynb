{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лабораторная работа #1.\n",
    " Реализовать скрапер по seed urls list, и сохранить текст + изображения с 200 страниц (в файлы на pc). Структура: 1 каталог, внутри которого текстовой файл + файлы с изображениями. А также отдельный файл с мета информацией (url, количественные характеристики загруженных данных)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_links(base_url):\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Извлечение всех ссылок на странице\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag.get('href')\n",
    "        absolute_url = urljoin(base_url, href)\n",
    "        # Проверка, что ссылка заканчивается на '.html' и содержит 'football'\n",
    "        # 201 потому что это костыль, у меня первая ссылка идёт с подссылкой не 'footaball', а 'mediafootball', а я не хочу этого\n",
    "        if absolute_url.endswith('.html') and 'football' in absolute_url and len(links) < 201:\n",
    "            links.append(absolute_url)\n",
    "        if len(links) >= 201:\n",
    "            break\n",
    "    \n",
    "    return links\n",
    "\n",
    "link_1 = 'https://www.sports.ru/football/news/'\n",
    "link_2 = 'https://www.espn.com/soccer/'\n",
    "link_3 = 'https://www.skysports.com/football'\n",
    "\n",
    "links_list_sports_ru = get_links(link_1)\n",
    "links_list_sports_ru = links_list_sports_ru[1:]\n",
    "print(len(links_list_sports_ru))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def fetch_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_page(html, base_url):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Извлечение текста\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    text = soup.get_text(separator=' ')\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Извлечение URL изображений\n",
    "    images = []\n",
    "    for img in soup.find_all('img'):\n",
    "        img_url = img.get('src')\n",
    "        if img_url:\n",
    "            img_url = urljoin(base_url, img_url)\n",
    "            images.append(img_url)\n",
    "    \n",
    "    return text, images\n",
    "\n",
    "def append_text(text, path):\n",
    "    with open(path, 'a', encoding='utf-8') as f:\n",
    "        f.write(text + '\\n\\n')\n",
    "        #print(len(text))\n",
    "\n",
    "def save_images(images, directory, start_index):\n",
    "    # for idx, img_url in enumerate(images):\n",
    "    #     try:\n",
    "    #         img_data = requests.get(img_url).content\n",
    "    #         img_name = os.path.join(directory, f'image_{idx + 1}.jpg')\n",
    "    #         with open(img_name, 'wb') as f:\n",
    "    #             f.write(img_data)\n",
    "    #     except requests.RequestException as e:\n",
    "    #         print(f\"Error saving image {img_url}: {e}\")\n",
    "    image_index = start_index\n",
    "    for img_url in images:\n",
    "        try:\n",
    "            img_data = requests.get(img_url).content\n",
    "            img_name = os.path.join(directory, f'image_{image_index}.jpg')\n",
    "            with open(img_name, 'wb') as f:\n",
    "                f.write(img_data)\n",
    "            image_index += 1\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error saving image {img_url}: {e}\")\n",
    "    return image_index\n",
    "\n",
    "def scrape(seed_urls, output_directory, max_pages=5):\n",
    "    create_directory(output_directory)\n",
    "    \n",
    "    meta_info = []\n",
    "    pages_scraped = 0\n",
    "    image_index = 1\n",
    "    sum_page_images, sum_len_page_text = 0, 0\n",
    "    text_file_path = os.path.join(output_directory, 'content.txt')\n",
    "    \n",
    "    for url in seed_urls:\n",
    "        if pages_scraped >= max_pages:\n",
    "            break\n",
    "\n",
    "        page_html = fetch_page(url)\n",
    "        if page_html:\n",
    "            page_text, page_images = parse_page(page_html, url)\n",
    "            \n",
    "            # Сохранение текста и изображений\n",
    "            append_text(page_text, text_file_path)\n",
    "            image_index = save_images(page_images, output_directory, image_index)\n",
    "            \n",
    "            # Сохранение метаинформации\n",
    "            sum_len_page_text += len(page_text)\n",
    "            sum_page_images += len(page_images)\n",
    "            \n",
    "            meta_info.append({\n",
    "                'url': url,\n",
    "                'text_length': len(page_text),\n",
    "                'num_images': len(page_images)\n",
    "            })\n",
    "            \n",
    "            pages_scraped += 1\n",
    "    \n",
    "    # Запись метаинформации в файл\n",
    "    with open(os.path.join('scraped_data', 'meta_info_www_sports_ru.txt'), 'w', encoding='utf-8') as f:\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        f.write(f\"{current_time}\\n\")\n",
    "        f.write(f\"Sum Text Length: {sum_len_page_text}\\n\")\n",
    "        f.write(f\"Sum Number of Images: {sum_page_images}\\n\\n\")\n",
    "        for info in meta_info:\n",
    "            f.write(f\"URL: {info['url']}\\n\")\n",
    "            f.write(f\"Text Length: {info['text_length']}\\n\")\n",
    "            f.write(f\"Number of Images: {info['num_images']}\\n\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "seed_urls = links_list_sports_ru\n",
    "\n",
    "output_directory = 'scraped_data/www_sports_ru'\n",
    "scrape(seed_urls, output_directory, max_pages=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae319d6ce38a8e735e7a53cda922e94f1f104ddd22f27ee36ddcd6f27dd0c89a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
